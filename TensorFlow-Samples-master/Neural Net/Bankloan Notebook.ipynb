{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "      <th>preddef1</th>\n",
       "      <th>preddef2</th>\n",
       "      <th>preddef3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808394</td>\n",
       "      <td>0.788640</td>\n",
       "      <td>0.213043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198297</td>\n",
       "      <td>0.128445</td>\n",
       "      <td>0.436903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>55</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.141023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.104422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781588</td>\n",
       "      <td>0.737885</td>\n",
       "      <td>0.436903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  ed  employ  address  income  debtinc   creddebt   othdebt default  \\\n",
       "0   41   3      17       12     176      9.3  11.359392  5.008608       1   \n",
       "1   27   1      10        6      31     17.3   1.362202  4.000798       0   \n",
       "2   40   1      15       14      55      5.5   0.856075  2.168925       0   \n",
       "3   41   1      15       14     120      2.9   2.658720  0.821280       0   \n",
       "4   24   2       2        0      28     17.3   1.787436  3.056564       1   \n",
       "\n",
       "   preddef1  preddef2  preddef3  \n",
       "0  0.808394  0.788640  0.213043  \n",
       "1  0.198297  0.128445  0.436903  \n",
       "2  0.010036  0.002987  0.141023  \n",
       "3  0.022138  0.010273  0.104422  \n",
       "4  0.781588  0.737885  0.436903  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data_1 = pd.read_csv('bankloanData.csv')\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "ed            int64\n",
       "employ        int64\n",
       "address       int64\n",
       "income        int64\n",
       "debtinc     float64\n",
       "creddebt    float64\n",
       "othdebt     float64\n",
       "default      object\n",
       "preddef1    float64\n",
       "preddef2    float64\n",
       "preddef3    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41, 3, 17, ..., 0.808394327359702, 0.7886404318214371,\n",
       "        0.21304337612811897],\n",
       "       [27, 1, 10, ..., 0.19829747615910395, 0.128445387038174,\n",
       "        0.43690300550604605],\n",
       "       [40, 1, 15, ..., 0.0100361080990023, 0.00298677834821412,\n",
       "        0.141022623460993],\n",
       "       ..., \n",
       "       [48, 1, 13, ..., 0.0301374981044824, 0.0325702625943738,\n",
       "        0.24801041775523303],\n",
       "       [35, 2, 1, ..., 0.26900345101699397, 0.37854649636973203,\n",
       "        0.181814378077261],\n",
       "       [37, 1, 20, ..., 0.006397812918809229, 0.0111731232851226,\n",
       "        0.30304155578236497]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make a numpy array from the dataframe, except remove rows with no value for 'default'\n",
    "i = list(df_data_1.columns.values).index('default')\n",
    "data = np.array([x for x in df_data_1.values if x[i] in ['0', '1']])\n",
    "\n",
    "# Remove the columns for preddef1, predef2 and preddef3\n",
    "data = np.delete(data, slice(9,12), axis=1)\n",
    "\n",
    "# Separate the 'predictors' (aka 'features') from the dependent variable (aka 'label') \n",
    "# that we will learn how to predict\n",
    "predictors = np.delete(data, 8, axis=1)\n",
    "dependent = np.delete(data, slice(0, 8), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the label type to numeric categorical representing the classes to predict (binary classfier)\n",
    "dependent = dependent.astype(int)\n",
    "\n",
    "# And flatten it to one dimensional for use as the expected output label vector in TensorFlow\n",
    "dependent = dependent.flatten()\n",
    "dependent\n",
    "\n",
    "# Convert all the predictors to float to simplify this demo TensorFlow code\n",
    "predictors = predictors.astype(float)\n",
    "\n",
    "# Get the shape of the predictors\n",
    "m, n = predictors.shape\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  41.      ,    3.      ,   17.      ,   12.      ,  176.      ,\n",
       "          9.3     ,   11.359392,    5.008608])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partition the input data into a training set and a test set\n",
    "\n",
    "m_train = 500\n",
    "m_test = m - m_train\n",
    "\n",
    "predictors_train = predictors[:m_train]\n",
    "dependent_train = dependent[:m_train]\n",
    "\n",
    "predictors_test = predictors[m_train:]\n",
    "dependent_test = dependent[m_train:]\n",
    "\n",
    "# Gets a batch of the training data. \n",
    "# NOTE: Rather than loading a whole large data set as above and then taking array slices as done here, \n",
    "#       This method can connect to a data source and select just the batch needed.\n",
    "def get_training_batch(batch_num, batch_size):\n",
    "    lower = batch_num * (m_train // batch_size)\n",
    "    upper = lower + batch_size\n",
    "    return predictors_train[lower:upper], dependent_train[lower:upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Make this notebook's output stable across runs\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# A method to build a new neural net layer of a given size,  \n",
    "# fully connect it to a given preceding layer X, and \n",
    "# compute its output Z either with or without (default) an activation function\n",
    "# Call with activation=tf.nn.relu or tf.nn.sigmoid or tf.nn.tanh, for examples\n",
    "\n",
    "def make_nn_layer(layer_name, layer_size, X, activation=None):\n",
    "    with tf.name_scope(layer_name):\n",
    "        X_size = int(X.get_shape()[1])\n",
    "        SD = 2 / np.sqrt(X_size)\n",
    "        weights = tf.truncated_normal((X_size, layer_size), dtype=tf.float64, stddev=SD)\n",
    "        W = tf.Variable(weights, name='weights')\n",
    "        b = tf.Variable(tf.zeros([layer_size], dtype=tf.float64), name='biases')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the neural net structure\n",
    "\n",
    "n_inputs = n\n",
    "n_hidden1 = n \n",
    "### n_hidden2 = n // 2\n",
    "n_outputs = 2   # Two output classes: defaulting or non-defaulting on loan\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=(None, n_inputs), name='X')\n",
    "\n",
    "with tf.name_scope('nn'):\n",
    "    hidden1 = make_nn_layer('hidden1', n_hidden1, X, activation=tf.nn.relu)\n",
    "    hidden2 = hidden1\n",
    "    ### hidden2 = make_nn_layer('hidden2', n_hidden2, hidden1, activation=tf.nn.relu)\n",
    "    outputs = make_nn_layer('outputs', n_outputs, hidden2) \n",
    "    outputs = tf.identity(outputs, \"nn_output\")\n",
    "    \n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define how the neural net will learn\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "    loss = tf.reduce_mean(xentropy, name='l')\n",
    "    \n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"test\"):\n",
    "    correct = tf.nn.in_top_k(tf.cast(outputs, tf.float32), y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the ability to save and restore the trained neural net...\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ../datasets: File exists\n",
      "mkdir: ../datasets/Neural Net: File exists\n"
     ]
    }
   ],
   "source": [
    "# ... and make a subdirectory space in which to save the model files (only need to run this once)\n",
    "!mkdir \"../datasets\"\n",
    "!mkdir \"../datasets/Neural Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'Training accuracy:', 0.75999999, 'Testing accuracy:', 0.755)\n",
      "(200, 'Training accuracy:', 0.77399999, 'Testing accuracy:', 0.79000002)\n",
      "(300, 'Training accuracy:', 0.79799998, 'Testing accuracy:', 0.81999999)\n",
      "(400, 'Training accuracy:', 0.79799998, 'Testing accuracy:', 0.83499998)\n",
      "(500, 'Training accuracy:', 0.80800003, 'Testing accuracy:', 0.82499999)\n",
      "(600, 'Training accuracy:', 0.796, 'Testing accuracy:', 0.815)\n",
      "(700, 'Training accuracy:', 0.80199999, 'Testing accuracy:', 0.81)\n",
      "(800, 'Training accuracy:', 0.78799999, 'Testing accuracy:', 0.815)\n",
      "(900, 'Training accuracy:', 0.81, 'Testing accuracy:', 0.815)\n",
      "(1000, 'Training accuracy:', 0.81999999, 'Testing accuracy:', 0.81)\n",
      "(1100, 'Training accuracy:', 0.81800002, 'Testing accuracy:', 0.81)\n",
      "(1200, 'Training accuracy:', 0.80599999, 'Testing accuracy:', 0.80500001)\n",
      "(1300, 'Training accuracy:', 0.80199999, 'Testing accuracy:', 0.815)\n",
      "(1400, 'Training accuracy:', 0.81199998, 'Testing accuracy:', 0.81999999)\n",
      "(1500, 'Training accuracy:', 0.80599999, 'Testing accuracy:', 0.80000001)\n",
      "(1600, 'Training accuracy:', 0.81599998, 'Testing accuracy:', 0.81)\n",
      "(1700, 'Training accuracy:', 0.80599999, 'Testing accuracy:', 0.81)\n",
      "(1800, 'Training accuracy:', 0.82200003, 'Testing accuracy:', 0.82499999)\n",
      "(1900, 'Training accuracy:', 0.80199999, 'Testing accuracy:', 0.80500001)\n",
      "(2000, 'Training accuracy:', 0.82999998, 'Testing accuracy:', 0.81999999)\n",
      "(2100, 'Training accuracy:', 0.81199998, 'Testing accuracy:', 0.81)\n",
      "(2200, 'Training accuracy:', 0.824, 'Testing accuracy:', 0.82499999)\n",
      "(2300, 'Training accuracy:', 0.80400002, 'Testing accuracy:', 0.815)\n",
      "(2400, 'Training accuracy:', 0.81800002, 'Testing accuracy:', 0.815)\n",
      "(2500, 'Training accuracy:', 0.82800001, 'Testing accuracy:', 0.82499999)\n",
      "(2600, 'Training accuracy:', 0.82999998, 'Testing accuracy:', 0.815)\n",
      "(2700, 'Training accuracy:', 0.82200003, 'Testing accuracy:', 0.815)\n",
      "(2800, 'Training accuracy:', 0.81999999, 'Testing accuracy:', 0.80500001)\n",
      "(2900, 'Training accuracy:', 0.81400001, 'Testing accuracy:', 0.82999998)\n",
      "(3000, 'Training accuracy:', 0.82599998, 'Testing accuracy:', 0.82499999)\n",
      "\n",
      "('Actual classes:   ', array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]))\n",
      "('Predicted classes:', array([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# TRAINING TIME\n",
    "\n",
    "# This is how many times to use the full set of training data\n",
    "n_epochs = 3000\n",
    "\n",
    "# For a larger training set, it's typically necessary to break training into\n",
    "# batches so only the memory needed to store one batch of training data is used\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as training_session:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Shuffling (across batches) is easier to do for small data sets and\n",
    "        # helps increase accuracy\n",
    "        training_set = [[pt_elem, dependent_train[i]] for i, pt_elem in enumerate(predictors_train)]\n",
    "        np.random.shuffle(training_set)\n",
    "        predictors_train = [ts_elem[0] for ts_elem in training_set]\n",
    "        dependent_train = [ts_elem[1] for ts_elem in training_set]\n",
    "        \n",
    "        # Loop through the whole training set in batches\n",
    "        for batch_num in range(m_train // batch_size):\n",
    "            X_batch, y_batch = get_training_batch(batch_num, batch_size)\n",
    "            training_session.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        if epoch % 100 == 99:\n",
    "            acc_train = accuracy.eval(feed_dict={X: predictors_train, y: dependent_train})\n",
    "            acc_test = accuracy.eval(feed_dict={X: predictors_test, y: dependent_test})\n",
    "            print(epoch+1, \"Training accuracy:\", acc_train, \"Testing accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(training_session, \"../datasets/Neural Net/Neural Net.ckpt\")\n",
    "    \n",
    "    # A quick test with the trained model \n",
    "    Z = outputs.eval(feed_dict={X: predictors_test[:20]})\n",
    "    dependent_pred = np.argmax(Z, axis=1)\n",
    "    print(\"\")\n",
    "    print(\"Actual classes:   \", dependent_test[:20])  \n",
    "    print(\"Predicted classes:\", dependent_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../datasets/Neural Net/Neural Net.ckpt\n",
      "('Actual classes:   ', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1]))\n",
      "('Predicted classes:', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1]))\n",
      "\n",
      "('Confidences: ', [0.72115788841227302, 0.80607047578305713, 0.83934041973914708, 0.99522251895376912, 0.63583707224033514, 0.92301370061024191, 0.9246727235093638, 0.78439759502501416, 0.99999999999583422, 0.64561868763996888, 0.72736408931601459, 0.64561868763996888, 0.83959328566375235, 0.94291801514712859, 0.68170960394665236, 0.99981345698850999, 0.96778595641239296, 0.59248611260528017, 0.99293831799558985, 0.52204835458766652])\n",
      "\n",
      "('Probabilities: ', array([[  7.21157888e-01,   2.78842112e-01],\n",
      "       [  8.06070476e-01,   1.93929524e-01],\n",
      "       [  8.39340420e-01,   1.60659580e-01],\n",
      "       [  9.95222519e-01,   4.77748105e-03],\n",
      "       [  6.35837072e-01,   3.64162928e-01],\n",
      "       [  9.23013701e-01,   7.69862994e-02],\n",
      "       [  9.24672724e-01,   7.53272765e-02],\n",
      "       [  7.84397595e-01,   2.15602405e-01],\n",
      "       [  1.00000000e+00,   4.16586334e-12],\n",
      "       [  3.54381312e-01,   6.45618688e-01],\n",
      "       [  7.27364089e-01,   2.72635911e-01],\n",
      "       [  3.54381312e-01,   6.45618688e-01],\n",
      "       [  8.39593286e-01,   1.60406714e-01],\n",
      "       [  9.42918015e-01,   5.70819849e-02],\n",
      "       [  6.81709604e-01,   3.18290396e-01],\n",
      "       [  9.99813457e-01,   1.86543011e-04],\n",
      "       [  9.67785956e-01,   3.22140436e-02],\n",
      "       [  4.07513887e-01,   5.92486113e-01],\n",
      "       [  9.92938318e-01,   7.06168200e-03],\n",
      "       [  4.77951645e-01,   5.22048355e-01]]))\n"
     ]
    }
   ],
   "source": [
    "# Restore the saved model and use it to perform inference on a \"received\" new set of data\n",
    "\n",
    "# We will simulate \"receiving\" the new data by taking a slice of the test set.\n",
    "##predictors_received = predictors_test[20:40]\n",
    "predictors_received = predictors_test[:]\n",
    "\n",
    "import tensorflow as tf_inference\n",
    "\n",
    "with tf_inference.Session() as inference_session:\n",
    "    inf_saver = tf_inference.train.import_meta_graph('../datasets/Neural Net/Neural Net.ckpt.meta')\n",
    "    inf_saver.restore(inference_session, tf_inference.train.latest_checkpoint('../datasets/Neural Net/'))\n",
    "    \n",
    "    graph = tf_inference.get_default_graph()\n",
    "    X = graph.get_tensor_by_name(\"X:0\")\n",
    "    nn_output = graph.get_tensor_by_name(\"nn/nn_output:0\")\n",
    "\n",
    "    Z = inference_session.run(nn_output, feed_dict={X: predictors_received})\n",
    "    dependent_pred = np.argmax(Z, axis=1)\n",
    "    \n",
    "    dependent_prob = inference_session.run(tf_inference.nn.softmax(nn_output), feed_dict={X: predictors_received})\n",
    "\n",
    "    confidences = [p[dependent_pred[i]] for i, p in enumerate(dependent_prob)]\n",
    "    \n",
    "print(\"Actual classes:   \", dependent_test[20:40])\n",
    "## print(\"Predicted classes:\", dependent_pred)\n",
    "print(\"Predicted classes:\", dependent_pred[20:40])\n",
    "print(\"\")\n",
    "## print(\"Confidences: \", confidences)\n",
    "print(\"Confidences: \", confidences[20:40])\n",
    "print(\"\")\n",
    "## print(\"Probabilities: \", dependent_prob)\n",
    "print(\"Probabilities: \", dependent_prob[20:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 80\r\n",
      "-rw-r--r--  1 boyerj  staff    720 13 Feb 17:38 Neural Net.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r--  1 boyerj  staff    238 13 Feb 17:38 Neural Net.ckpt.index\r\n",
      "-rw-r--r--  1 boyerj  staff  25750 13 Feb 17:38 Neural Net.ckpt.meta\r\n",
      "-rw-r--r--  1 boyerj  staff     87 13 Feb 17:38 checkpoint\r\n"
     ]
    }
   ],
   "source": [
    "# List the model files\n",
    "!ls -l \"../datasets/Neural Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85273972602739723"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we're going to assess the quality of the neural net using ROC curve and AUC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# send the actual dependent variable classifications for param 1, \n",
    "# and the confidences of the true classification for param 2.\n",
    "FPR, TPR, _ = roc_curve(dependent_test, dependent_prob[:, 1])\n",
    "\n",
    "# Calculate the area under the confidence ROC curve.\n",
    "# This area is equated with the probability that the classifier will rank \n",
    "# a randomly selected defaulter higher than a randomly selected non-defaulter.\n",
    "AUC = auc(FPR, TPR)\n",
    "\n",
    "# What is \"good\" can dependm but an AUC of 0.7+ is generally regarded as good, \n",
    "# and 0.8+ is generally regarded as being excellent \n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e4b61d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we'll plot the confidence ROC curve \n",
    "plt.figure()\n",
    "plt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % AUC)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.02])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.91      0.88       146\n",
      "          1       0.71      0.59      0.65        54\n",
      "\n",
      "avg / total       0.82      0.82      0.82       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(dependent_test, dependent_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043730491447763684"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Say you want a model that is very accurate at recalling true positives (defaulters), \n",
    "# even if it gets a lot a false positives (non-defaulters). You might be automatically\n",
    "# excepting the false classifications and, for true classifications, you may send them\n",
    "# for human review rather than rejecting their loan applications.\n",
    "\n",
    "# The lowest confidence that can give 100% TPR on the test set is equal to the \n",
    "# true class with the lowest confidence, so we'll find that now\n",
    "defaulter_probs = [dependent_prob[i][1] for i, p in enumerate(dependent_test) if p == 1]\n",
    "min_conf = np.min(defaulter_probs)\n",
    "min_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each non-defaulter with a confidence at or above min_conf would be predicted \n",
    "# to be a defaulter (which would be a false positve prediction for a non-defaulter)\n",
    "\n",
    "non_defaulter_probs = [dependent_prob[i][1] for i, p in enumerate(dependent_test) if p == 0]\n",
    "false_positives = [x for x in non_defaulter_probs if x >= min_conf]\n",
    "\n",
    "total = len(defaulter_probs) + len(non_defaulter_probs)\n",
    "total_correct = total - len(false_positives)\n",
    "accuracy = float(total_correct) / total\n",
    "\n",
    "# Overall accuracy would suffer quite a bit, but this achieves \n",
    "# the desired high accuracy on true positive identification (defaulters)  \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For when you want to wipe out the training and do it again\n",
    "# !rm -rf \"../datasets/Neural Net\"\n",
    "## !rm -rf \"../datasets\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
